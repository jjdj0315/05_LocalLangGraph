{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c3457",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-3-12b-it\"\n",
    "print(f\"# Model ID: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e86567",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# 양자화 옵션: 4비트\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # nf4 양자화\n",
    "\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\", # 계산시에는 기존 자료형 bf16 사용\n",
    "    bnb_4bit_use_double_quant=True # 이중 양자화\n",
    ")\n",
    "\n",
    "\n",
    "# 토크나이저와 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "\n",
    "    torch_dtype=torch.bfloat16, # Gemma 3은 bf16 모델입니다!\n",
    "    device_map={\"\":0},  # 0번 GPU에 할당\n",
    "\n",
    "    quantization_config = quantization_config,\n",
    "    # 양자화 불필요시 제거\n",
    "\n",
    "    attn_implementation = 'eager'\n",
    "    # flash attention 설정\n",
    "    # Gemma Series가 아닌 경우 'eager'를 \"flash_attention_2\" 로 변경\n",
    "    # 미지원/사용하지 않는 경우 제거 (T4 등의 구버전 GPU 등)\n",
    "\n",
    ")\n",
    "\n",
    "# if hasattr(model, 'language_model'):\n",
    "#     model = model.language_model\n",
    "# Multimodal 모델의 경우, Language Model만 선택할 수도 있으나\n",
    "# 비전 모델의 사이즈가 매우 작기 때문에 큰 차이는 없음\n",
    "\n",
    "# Train X (eval)\n",
    "model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
