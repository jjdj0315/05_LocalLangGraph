{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsYrzxX5DaqK"
      },
      "source": [
        "# [ì‹¤ìŠµ] HuggingFace ê³µê°œ LLMì„ í™œìš©í•œ Agentic Work ë§Œë“¤ê¸°\n",
        "\n",
        "HuggingFaceì— ê²Œì‹œëœ ê³µê°œ ëª¨ë¸ë“¤ì˜ ê²½ìš°, `bind_tools`ì™€ ê°™ì€ ê¸°ëŠ¥ë“¤ì´ ì œëŒ€ë¡œ ì—°ë™ë˜ì§€ ì•Šê±°ë‚˜,   \n",
        "ìì²´ì ìœ¼ë¡œ Tool Calling ê¸°ëŠ¥ì´ ì—†ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.   \n",
        "\n",
        "ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, Tool Calling ë°©ë²•ì„ ì§ì ‘ êµ¬ì„±í•˜ê³  Agentic Workë¥¼ êµ¬í˜„í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiuZNYFq14aN"
      },
      "source": [
        "## ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "\n",
        "\n",
        "ì´ë²ˆ ì‹¤ìŠµì€ ë¬´ë£Œ ì½”ë©(T4 GPU)ì´ ì•„ë‹Œ ê³ ì„±ëŠ¥ GPU ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì§„í–‰í•©ë‹ˆë‹¤.   \n",
        "ë¬´ë£Œ ì½”ë©ìœ¼ë¡œ ì§„í–‰í•˜ì‹œëŠ” ê²½ìš°, GPU ì„±ëŠ¥ì˜ í•œê³„ë¡œ ì‹¤í–‰ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.   \n",
        "(ì•„ë˜ ì½”ë“œì—ì„œ, **T4 ì‚¬ìš©ì‹œ í•´ì œ** ë¶€ë¶„ì„ ì°¸ê³ í•˜ì„¸ìš”!)\n",
        "\n",
        "<br><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Udtv--pI14aN",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers bitsandbytes accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1JC3bK9DaqL"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade langchain_experimental langgraph langchain langchain_community langchain_huggingface dotenv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1eMoe-G14aO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gYEfMXn14aO",
        "outputId": "276917bd-6172-4f0f-f252-d3126f7288fb"
      },
      "outputs": [],
      "source": [
        "# Flash Attention: ë¦¬ëˆ…ìŠ¤ ì „ìš© ì„¤ì¹˜ë°©ë²•\n",
        "# ë¬´ë£Œ ì½”ë©(T4)ì—ì„œëŠ” ì„¤ì¹˜ X\n",
        "\n",
        "# Windows ì„¤ì¹˜ëŠ” https://github.com/kingbri1/flash-attention/releases ì°¸ê³ \n",
        "!pip install flash-attn --no-build-isolation -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DtuBpmP14aO"
      },
      "source": [
        "ì„¤ì¹˜í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë§ìœ¼ë¯€ë¡œ, ê°€ê¸‰ì  ì„¤ì¹˜ í›„ ì„¸ì…˜ ì¬ì‹œì‘ì„ ìˆ˜í–‰í•´ ì£¼ì„¸ìš”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBhW7vFc14aO"
      },
      "source": [
        "í—ˆê¹…í˜ì´ìŠ¤ í† í°ì„ ì¸ì¦í•©ë‹ˆë‹¤.    \n",
        "\n",
        "https://huggingface.co/settings/tokens ì—ì„œ Read ê¶Œí•œ í† í°ì„ ë°œê¸‰ë°›ìŠµë‹ˆë‹¤.   \n",
        "ì•„ë˜ ì½”ë“œì—ì„œëŠ”, env íŒŒì¼ì— `HF_READ_TOKEN`ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKRxTZ5n14aO"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVFaB-AA14aO",
        "outputId": "047ac881-646b-4d8f-93d6-a848a719e239"
      },
      "outputs": [],
      "source": [
        "load_dotenv('env') #.env íŒŒì¼ì¸ ê²½ìš° .envë¡œ ë³€ê²½"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "siIa3Voy14aO",
        "outputId": "04a4ff69-9a66-4687-fb4f-b3733570a545"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJEt8RpX14aO",
        "outputId": "0194c900-3ae1-479a-f954-2e64d8a5d1e8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "# í—ˆê¹…í˜ì´ìŠ¤ í† í° ë¡œê·¸ì¸\n",
        "login(token=os.environ['HF_TOKEN'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d__aPk5_14aO"
      },
      "source": [
        "### 1. Gemma 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrzEKTP514aP"
      },
      "source": [
        "ë¶ˆëŸ¬ì˜¬ ëª¨ë¸ì€ êµ¬ê¸€ì˜ Gemma-3-12b-it ì…ë‹ˆë‹¤.   \n",
        "ì‹¤ì œ í¬ê¸°ëŠ” 24GB ì •ë„ì´ì§€ë§Œ, 4Bit ì–‘ìí™”ë¥¼ í†µí•´ 8~9GB í¬ê¸°ë¡œ ë¡œë“œí•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6QY8RHv14aP",
        "outputId": "a5be359d-d34b-48ba-c51e-99da2394aa45"
      },
      "outputs": [],
      "source": [
        "model_id = \"google/gemma-3-12b-it\"\n",
        "print(f\"# Model ID: {model_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE5W5Bat14aP"
      },
      "source": [
        "íŠ¸ëœìŠ¤í¬ë¨¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z592IDzIDaqM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# ì–‘ìí™” ì˜µì…˜: 4ë¹„íŠ¸\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    # nf4 ì–‘ìí™”\n",
        "\n",
        "    bnb_4bit_compute_dtype=\"bfloat16\", # ê³„ì‚°ì‹œì—ëŠ” ê¸°ì¡´ ìë£Œí˜• bf16 ì‚¬ìš©\n",
        "    bnb_4bit_use_double_quant=True # ì´ì¤‘ ì–‘ìí™”\n",
        ")\n",
        "\n",
        "\n",
        "# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "\n",
        "    torch_dtype=torch.bfloat16, # Gemma 3ì€ bf16 ëª¨ë¸ì…ë‹ˆë‹¤!\n",
        "    device_map={\"\":0},  # 0ë²ˆ GPUì— í• ë‹¹\n",
        "\n",
        "    quantization_config = quantization_config,\n",
        "    # ì–‘ìí™” ë¶ˆí•„ìš”ì‹œ ì œê±°\n",
        "\n",
        "    attn_implementation = 'eager'\n",
        "    # flash attention ì„¤ì •\n",
        "    # Gemma Seriesê°€ ì•„ë‹Œ ê²½ìš° 'eager'ë¥¼ \"flash_attention_2\" ë¡œ ë³€ê²½\n",
        "    # ë¯¸ì§€ì›/ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì œê±° (T4 ë“±ì˜ êµ¬ë²„ì „ GPU ë“±)\n",
        "\n",
        ")\n",
        "\n",
        "# if hasattr(model, 'language_model'):\n",
        "#     model = model.language_model\n",
        "# Multimodal ëª¨ë¸ì˜ ê²½ìš°, Language Modelë§Œ ì„ íƒí•  ìˆ˜ë„ ìˆìœ¼ë‚˜\n",
        "# ë¹„ì „ ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆê°€ ë§¤ìš° ì‘ê¸° ë•Œë¬¸ì— í° ì°¨ì´ëŠ” ì—†ìŒ\n",
        "\n",
        "# Train X (eval)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRYqBlyQ14aP"
      },
      "source": [
        "ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¨ ë’¤ì—ëŠ” Text-Generation íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.   \n",
        "ì´ ë•Œ, ì ì ˆí•œ ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ê¸° ìœ„í•´ ëª¨ë¸ í™ˆí˜ì´ì§€ë¥¼ ì°¸ê³ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gFbGhuK14aP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
        "\n",
        "# Sampling íŒŒë¼ë¯¸í„° ì„¤ì •: https://huggingface.co/google/gemma-3-12b-it\n",
        "# í™ˆí˜ì´ì§€ ê¶Œì¥ ì‚¬ì–‘ì´ì§€ë§Œ, ì ì ˆí•˜ê²Œ ë°”ê¿”ë„ ë¨\n",
        "gen_config = dict(\n",
        "    do_sample=True,\n",
        "    max_new_tokens=2048,\n",
        "    repetition_penalty = 1.05,\n",
        "\n",
        "    temperature = 1.0,\n",
        "    top_p = 0.95,\n",
        "    top_k = 64\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    # return_full_text=True <-- ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ í¬í•¨í•œ ì „ì²´ ì¶œë ¥í•˜ê¸°\n",
        "    **gen_config\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a-z0Pbt14aP"
      },
      "source": [
        "ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ë¥¼ í™•ì¸í•˜ì—¬, ì±„íŒ… í…œí”Œë¦¿ êµ¬ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤.   \n",
        "ì „ì²´ë¥¼ ì´í•´í•  í•„ìš”ëŠ” ì—†ì§€ë§Œ, Toolì´ë‚˜ Functionì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRHzfqUk14aP"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.chat_template)\n",
        "# Tool ë¯¸ì§€ì› ğŸ˜£"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqWVrzLe14aP"
      },
      "source": [
        "ëª¨ë¸ì„ ë­ì²´ì¸ê³¼ ì—°ë™í•©ë‹ˆë‹¤.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZsarwsm14aP"
      },
      "outputs": [],
      "source": [
        "base_llm = HuggingFacePipeline(pipeline=pipe, pipeline_kwargs=gen_config)\n",
        "# base_llm: ì…ë ¥ ì „ Tokenizerë¡œ í…œí”Œë¦¿ ë³€í™˜ í•„ìˆ˜, ìŠ¤íŠ¸ë¦¬ë° ê°€ëŠ¥\n",
        "\n",
        "llm = ChatHuggingFace(llm=base_llm, tokenizer=tokenizer)\n",
        "# llm: ìë™ í…œí”Œë¦¿ ë³€í™˜(ChatGoogleGenAIì™€ ë™ì¼), ìŠ¤íŠ¸ë¦¬ë° ë¶ˆê°€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPs0pT9z14aP"
      },
      "outputs": [],
      "source": [
        "def convert_chat(messages, add_generation_prompt = True):\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
        "\n",
        "example = [{'role':'user', 'content':'ì•ˆë…•'}]\n",
        "convert_chat(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tf93bsuV14aQ"
      },
      "outputs": [],
      "source": [
        "for s in base_llm.stream(convert_chat(example)):\n",
        "    print(s, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG9dLxzP14aQ"
      },
      "outputs": [],
      "source": [
        "llm.invoke(\"ì•ˆë…•?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwnCxMeP14aQ"
      },
      "source": [
        "## 2. HuggingFace LLMê³¼ íˆ´ ì—°ë™í•˜ê¸°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc0lmLMh14aQ"
      },
      "source": [
        "ë¨¼ì € íˆ´ì„ ì„¤ì •í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxxyvxZn14aQ"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.tools import tool\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from datetime import datetime\n",
        "\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=3)\n",
        "\n",
        "repl_tool = PythonREPLTool()\n",
        "repl_tool.invoke(\"for i in range(10): print(i)\")\n",
        "\n",
        "\n",
        "@tool\n",
        "def current_date() -> str:\n",
        "    \"í˜„ì¬ ë‚ ì§œë¥¼ %y-%m-%d í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "\n",
        "tools = [tavily_search, repl_tool, current_date]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPAdZG1H14aQ"
      },
      "source": [
        "`ChatHuggingFace()`ì—ë„ `bind_tools()`ì´ êµ¬í˜„ë˜ì–´ ìˆìœ¼ë‚˜,   \n",
        "ì‹¤ì œë¡œ ì‹¤í–‰ë˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZoPFhiN14aQ"
      },
      "outputs": [],
      "source": [
        "llm_with_tools = llm.bind_tools(tools)\n",
        "llm_with_tools.invoke(\"ì˜¤ëŠ˜ ë‚ ì§œê°€ ì–´ë–»ê²Œ ë˜ë‹ˆ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwvRgFfO14aQ"
      },
      "outputs": [],
      "source": [
        "llm_with_tools.invoke(\"ë„ˆëŠ” ì–´ë–¤ íˆ´ì´ë‚˜ í•¨ìˆ˜ë¥¼ ê°€ì§€ê³  ìˆë‹ˆ?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rP-lWBN14aQ"
      },
      "source": [
        "ì´ëŸ° ê²½ìš°, íˆ´ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” ì§ì ‘ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ êµ¬ì„±í•´ì•¼ í•©ë‹ˆë‹¤.   \n",
        "Tool ì—­í•  ë˜í•œ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ User ì—­í• ì„ íˆ´ ëŒ€ìš©ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBkBjlRE14aQ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "llm_with_tools.kwargs['tools']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pirXIlE14aQ"
      },
      "source": [
        "íˆ´ ì„¤ëª…ì´ ë‹´ê¸´ ë¬¸ìì—´ì„ êµ¬ì„±í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAvTnfq614aQ"
      },
      "outputs": [],
      "source": [
        "tool_desc = str('\\n---\\n'.join([str(x) for x in llm_with_tools.kwargs['tools']]))\n",
        "print(tool_desc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AaY3Syl14aQ"
      },
      "source": [
        "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.   \n",
        "ì„±ëŠ¥ì— ë§¤ìš° ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹˜ë¯€ë¡œ, ì˜ì–´ë¡œ ì‘ì„±í–ˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMNnrs6s14aQ"
      },
      "outputs": [],
      "source": [
        "system_prompt = f'''\n",
        "You are a helpful assistant with tools below.\n",
        "You can decide whether to invoke any functions or not.\n",
        "If you decide to use any of tools.\n",
        "print name and required parameters of the tool within a json blob correctly.\n",
        "For python code, Return the object as a raw dictionary, without escaping quotes or newlines.\n",
        "\n",
        "for tool use: wrap your output within ```tool_code```.\n",
        "\n",
        "Example:\n",
        "```tool_code\n",
        "{{\"name\":'name of tool', \"arguments\":{{List of apparent argument and parameters}}}}\n",
        "```\n",
        "\n",
        "When the output of the tool is provided, it will be wrapped within ``tool_output```\n",
        "Answer accordingly from the result of the tool output.\n",
        "\n",
        "The question might need some sequential, multiple tool execution.\n",
        "Think Step by Step.\n",
        "\n",
        "The following tools are available:\n",
        "{tool_desc}'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl0MkeqQ14aR"
      },
      "source": [
        "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ë“¤ì–´ê°€ì•¼ í•˜ëŠ” ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "- Tool Format\n",
        "- Tool Call Format\n",
        "- Tool Result Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLS6guf414aV"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
        "\n",
        "\n",
        "messages = [SystemMessage(system_prompt),\n",
        "            HumanMessage('ì˜¤ëŠ˜ ë‚ ì§œê°€ ë©°ì¹ ì´ë‹ˆ?')]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izjgV8rS14aW"
      },
      "outputs": [],
      "source": [
        "\n",
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVRGa7X914aW"
      },
      "outputs": [],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6okfSfZe14aW"
      },
      "source": [
        "tool_codeë¥¼ ë°›ì•˜ìœ¼ë‹ˆ, í•´ë‹¹ ë‚´ìš©ì„ íŒŒì‹±í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnDI-EaK14aW"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "\n",
        "def parse_tool(text):\n",
        "    try:\n",
        "        text = text.split('```tool_code\\n')[1].split('\\n```')[0]\n",
        "        # tool_codeë¡œ wrapëœ ì¤‘ê°„ ì½”ë“œ ì¶”ì¶œ\n",
        "\n",
        "        parsed = ast.literal_eval(text)\n",
        "        # Dict í˜•íƒœì˜ ê°’ ë³€í™˜ (json loadì™€ ìœ ì‚¬)\n",
        "\n",
        "        name = parsed.get('name')\n",
        "        arguments = parsed.get('arguments', {})\n",
        "        # nameê³¼ argument return\n",
        "        return {'name':name, 'arguments':arguments}\n",
        "    except (ValueError, SyntaxError):\n",
        "        return None\n",
        "\n",
        "result = parse_tool(response.content)\n",
        "name,arguments = result['name'], result['arguments']\n",
        "name, arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgwuDSRE14aW"
      },
      "source": [
        "íˆ´ ì‹¤í–‰ì„ ì—°ê²°í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfNnf7x514aW"
      },
      "outputs": [],
      "source": [
        "# íˆ´ ì´ë¦„ê³¼ íˆ´ ì—°ê²°\n",
        "tool_dict = {tool.name: tool for tool in tools}\n",
        "\n",
        "def execute_tool(name, arguments):\n",
        "    # íˆ´ ì‹¤í–‰í•œ ë’¤ tool_outputìœ¼ë¡œ wrap\n",
        "    result = f'''```tool_output\n",
        "{tool_dict[name].invoke(arguments)}\n",
        "```'''\n",
        "    return result\n",
        "\n",
        "tool_result = execute_tool(**parse_tool(response.content))\n",
        "\n",
        "print(tool_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImCgtT-714aW"
      },
      "outputs": [],
      "source": [
        "messages.append(HumanMessage(tool_result))\n",
        "messages[1:]\n",
        "# ì§ˆë¬¸ + Tool ìš”ì²­ + Tool ê²°ê³¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIPtqA1514aW"
      },
      "outputs": [],
      "source": [
        "# ê²°ê³¼ í•´ì„\n",
        "response = llm.invoke(messages)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPtixRcX14aW"
      },
      "outputs": [],
      "source": [
        "messages = [SystemMessage(system_prompt),\n",
        "            HumanMessage('2025ë…„ 4ì›” ë°œí‘œëœ GPT-4.1 ëª¨ë¸ì´ ì–´ë–¤ ëª¨ë¸ì´ì•¼? í•œêµ­ì–´ë¡œ ì„¤ëª…í•´ì¤˜.')]\n",
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdLc7MVO14aW"
      },
      "outputs": [],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIOlRjoY14aW"
      },
      "outputs": [],
      "source": [
        "tool_result = execute_tool(**parse_tool(response.content))\n",
        "tool_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg51FbMB14aW"
      },
      "outputs": [],
      "source": [
        "messages[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwbD7jk214aX"
      },
      "outputs": [],
      "source": [
        "messages.append(HumanMessage(tool_result))\n",
        "response = llm.invoke(messages)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gojp1ieM14aX"
      },
      "source": [
        "ì¼ë°˜ì ì¸ ì…ì¶œë ¥ ê´€ê³„ì˜ íˆ´ì€ ì´ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   \n",
        "(ë§Œì•½, Python_REPLê³¼ ê°™ì´ argumentê°€ ë³µì¡í•œ íˆ´ì„ ìˆ˜í–‰í•˜ëŠ” ê²½ìš°ì—ëŠ”   \n",
        "ë³„ë„ì˜ í•¨ìˆ˜ë¡œ ë³€í™˜í•˜ê±°ë‚˜ ê²°ê³¼ë¬¼ì„ ìˆ˜ì •í•˜ëŠ” ì‘ì—…ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEPVMsmk14aX"
      },
      "source": [
        "í•´ë‹¹ êµ¬í˜„ì„ í†µí•´, ReAct Agent êµ¬ì¡°ë¥¼ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤.    \n",
        "bind_toolsê°€ ì—†ê¸° ë•Œë¬¸ì—, ê¸°ì¡´ì˜ Tool Messageë¥¼ ì‚¬ìš©í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP38zKqB14aX"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages : Annotated[list, add_messages]   # ë©”ì‹œì§€ ë§¥ë½ì„ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur7uj1AG14aX"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "tool_list = {tool.name: tool for tool in tools}\n",
        "# tool ëª©ë¡ dictë¡œ ìƒì„±\n",
        "\n",
        "def tool_node(state):\n",
        "    tool_outputs = []\n",
        "    tool_call_msgs = state['messages'][-1]\n",
        "    # ë§ˆì§€ë§‰ ë©”ì‹œì§€: íˆ´ ì½œë§ ë©”ì‹œì§€\n",
        "    if '```tool_code' in tool_call_msgs.content:\n",
        "        tool_result = execute_tool(**parse_tool(tool_call_msgs.content))\n",
        "        # tool ì‹¤í–‰ ê²°ê³¼ ì–»ê¸° (ê²°ê³¼ëŠ” ```tool_output```)\n",
        "        tool_outputs.append(HumanMessage(tool_result))\n",
        "\n",
        "    return {'messages': tool_outputs}\n",
        "\n",
        "def agent(state):\n",
        "    system_prompt = SystemMessage(f'''\n",
        "You are a helpful assistant with tools below.\n",
        "You can decide whether to invoke any functions or not.\n",
        "If you decide to use any of tools.\n",
        "print name and required parameters of the tool within a json blob correctly.\n",
        "For python code, Return the object as a raw dictionary, without escaping quotes or newlines.\n",
        "\n",
        "for tool use: wrap your output within ```tool_code```.\n",
        "\n",
        "Example:\n",
        "```tool_code\n",
        "{{\"name\":'name of tool', \"arguments\":{{List of apparent argument and parameters}}}}\n",
        "```\n",
        "\n",
        "When the output of the tool is provided, it will be wrapped within ``tool_output```\n",
        "Answer accordingly from the result of the tool output.\n",
        "\n",
        "The question might need some sequential, multiple tool execution.\n",
        "Think Step by Step.\n",
        "\n",
        "The following tools are available:\n",
        "{tool_desc}\n",
        "\n",
        "\n",
        "Answer in Korean.''')\n",
        "\n",
        "\n",
        "    response = llm.invoke([system_prompt] + state[\"messages\"])\n",
        "    return {'messages': response}\n",
        "\n",
        "def tool_needed(state):\n",
        "\n",
        "    last_msg = state['messages'][-1]\n",
        "    if '```tool_code' in last_msg.content: # íˆ´ ì½œë§ì´ í•„ìš”í•˜ë©´\n",
        "        return \"continue\"\n",
        "    else:\n",
        "        return \"finish\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tgTNu8f14aX"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"agent\", agent)\n",
        "builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "builder.add_edge(START, 'agent'),\n",
        "builder.add_conditional_edges(\"agent\",\n",
        "                              tool_needed,\n",
        "                               {\"continue\": \"tools\",\"finish\": END})\n",
        "builder.add_edge(\"tools\", \"agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIaHRhJq14aX"
      },
      "outputs": [],
      "source": [
        "graph = builder.compile()\n",
        "graph # ìƒì„±í•œ ê·¸ë˜í”„ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwtyHK-C14aX"
      },
      "outputs": [],
      "source": [
        "response = graph.invoke({'messages':[HumanMessage(content=\"ì˜¤ëŠ˜ ë‚ ì§œì— íƒœì–´ë‚œ ìœ ëª…ì¸ë“¤ ì¡°ì‚¬í•´ì„œ ì•Œë ¤ì¤˜.\")]})\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gDQQHN_14aX"
      },
      "source": [
        "Gemma 3ëŠ” Native Tool Callì„ ì§€ì›í•˜ëŠ” ëª¨ë¸ì€ ì•„ë‹™ë‹ˆë‹¤.  \n",
        "í•˜ì§€ë§Œ, ì–´ëŠ ì •ë„ í•¨ìˆ˜ ì‹¤í–‰ ëŠ¥ë ¥ì„ ë³´ì´ëŠ”ë°ìš”.   \n",
        "\n",
        "ë§ˆì°¬ê°€ì§€ë¡œ, Gemma 3 ì´ì™¸ì˜ LLM ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ë„,  \n",
        "ê°ê°ì˜ Tool ìŠ¤í™ì— ëŒ€í•œ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§„í–‰í•˜ì‹¤ ìˆ˜ ìˆìœ¼ë‚˜, ì„±ëŠ¥ì€ ì¡°ê¸ˆ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.   \n",
        "Ex) Phi-4 https://huggingface.co/microsoft/Phi-4-mini-instruct\n",
        "\n",
        "<br><br>\n",
        "ì´ë²ˆì—ëŠ” Qwen 2.5 7Bë¥¼ ì´ìš©í•´ ì§„í–‰í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "**ì„¸ì…˜ ì´ˆê¸°í™” í›„ ì—¬ê¸°ì„œë¶€í„° ë‹¤ì‹œ ì‹œì‘í•´ ì£¼ì„¸ìš”!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSelbmwB14aX",
        "outputId": "13751504-363e-4803-8251-dae0429caee1"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv('env')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxqnxWpk14aX",
        "outputId": "0b5b7820-8e92-44a6-f548-9bd2f2042406"
      },
      "outputs": [],
      "source": [
        "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "print(f\"# Model ID: {model_id}\")\n",
        "# Full Precision ë¡œë“œ: 15.6GB\n",
        "# GPU ë¶€ì¡±ì‹œ ì–‘ìí™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KNPHUQ5_rKt"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# ì–‘ìí™” ì˜µì…˜: 4ë¹„íŠ¸\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    # nf4 ì–‘ìí™”\n",
        "\n",
        "    bnb_4bit_compute_dtype=\"bfloat16\", # ê³„ì‚°ì‹œì—ëŠ” ê¸°ì¡´ ìë£Œí˜• bf16 ì‚¬ìš©\n",
        "    bnb_4bit_use_double_quant=True # ì´ì¤‘ ì–‘ìí™”\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668,
          "referenced_widgets": [
            "98c5923efc05402fa4b06d32ad1588ef",
            "daf5a315673147779de0b999a7d2315b",
            "7878d799f8074aa3a8fc164c0daeab04",
            "26999a37e67148c299927987210755b4",
            "9de1e5c8a8b04dc1891ec35e400e3ced",
            "5369d4cdad1e48a696c4545b36023c75",
            "6bdaf67b2c3042f7a9058b0a7262114d",
            "7fcd670f11ba4e218f009ce0baa01eef",
            "33e9fdbd4cc94f7a9dad92a150046c8e",
            "588182a5c01f4f81883486dc74b6469f",
            "a1deaade948a4ff6acb34df5ac5f26dd"
          ]
        },
        "id": "RnflkPZR14aY",
        "outputId": "c5833b90-b1c1-495c-f1f0-121ed8fd24dd"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "\n",
        "    torch_dtype='auto',\n",
        "    device_map={\"\":0},  # 0ë²ˆ GPUì— í• ë‹¹\n",
        "\n",
        "    quantization_config  = quantization_config,\n",
        "\n",
        "    # attn_implementation = 'flash_attention_2'\n",
        "    # flash attention ì„¤ì •\n",
        "    # ë¯¸ì§€ì›/ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì œê±° (T4 ë“±ì˜ êµ¬ë²„ì „ GPU ë“±)\n",
        "\n",
        ")\n",
        "\n",
        "# Train X (eval)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZshkwSU14aY",
        "outputId": "565ac542-e095-4101-a1ec-45ab0fd9848b"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
        "\n",
        "# Sampling íŒŒë¼ë¯¸í„° ì„¤ì •: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct\n",
        "# í™ˆí˜ì´ì§€ ê¶Œì¥ ì‚¬ì–‘ì´ì§€ë§Œ, ì ì ˆí•˜ê²Œ ë°”ê¿”ë„ ë¨\n",
        "gen_config = dict(\n",
        "    do_sample=True,\n",
        "    max_new_tokens=2048,\n",
        "    repetition_penalty = 1.05,\n",
        "\n",
        "    temperature = 0.1,\n",
        "    top_p = 0.8,\n",
        "    top_k = 20\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    # return_full_text=True <-- ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ í¬í•¨í•œ ì „ì²´ ì¶œë ¥í•˜ê¸°\n",
        "    **gen_config\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpGHbrzF14aY"
      },
      "source": [
        "Qwen 2.5 ëŠ” Tool ê¸°ëŠ¥ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsDwcaHq14aY",
        "outputId": "5f4f271d-638b-4816-f2af-ac46cad9a71f"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.chat_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDoXXYMK14aY"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.tools import tool\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from datetime import datetime\n",
        "\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=3)\n",
        "\n",
        "repl_tool = PythonREPLTool()\n",
        "\n",
        "\n",
        "@tool\n",
        "def current_date() -> str:\n",
        "    \"í˜„ì¬ ë‚ ì§œë¥¼ %y-%m-%d í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "\n",
        "tools = [tavily_search, repl_tool, current_date]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTcLxWoK14aY"
      },
      "outputs": [],
      "source": [
        "base_llm = HuggingFacePipeline(pipeline=pipe, pipeline_kwargs=gen_config)\n",
        "# base_llm: ì…ë ¥ ì „ Tokenizerë¡œ í…œí”Œë¦¿ ë³€í™˜ í•„ìˆ˜, ìŠ¤íŠ¸ë¦¬ë° ê°€ëŠ¥\n",
        "\n",
        "llm = ChatHuggingFace(llm=base_llm, tokenizer=tokenizer)\n",
        "# llm: ìë™ í…œí”Œë¦¿ ë³€í™˜(ChatGoogleGenAIì™€ ë™ì¼), ìŠ¤íŠ¸ë¦¬ë° ë¶ˆê°€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZhrctni14aY"
      },
      "source": [
        "íˆ´ ê¸°ëŠ¥ì´ ìˆì–´ë„, bind_toolsëŠ” ì‚¬ìš©ì´ ì–´ë µìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMKVBn3-14aY",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "llm_with_tools = llm.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BXWUzvg14aY",
        "outputId": "5ce776cf-867d-4083-ff71-cc350f3df970"
      },
      "outputs": [],
      "source": [
        "llm_with_tools.invoke(\"ì˜¤ëŠ˜ ë‚ ì§œê°€ ì–´ë–»ê²Œ ë˜ë‹ˆ? íˆ´ì„ ì‚¬ìš©í•´ì„œ ì•Œë ¤ì¤˜.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXByDlQD14aY"
      },
      "source": [
        "íˆ´ì„ json listë¡œ ì „ë‹¬í•˜ì—¬ í…œí”Œë¦¿ì— ì ìš©í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT-XCdOo14aY",
        "outputId": "53c65edf-1a1a-4325-d7a3-36370a8dbeba"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "tool_desc = llm_with_tools.kwargs['tools']\n",
        "print(tool_desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ_N2uSH14aY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sayymRP14aY"
      },
      "source": [
        "í† í¬ë‚˜ì´ì €ì— toolì„ ì „ë‹¬í•˜ì—¬, ì „ë°˜ì ì¸ í…œí”Œë¦¿ì„ í™•ì¸í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAYKx-ca14aZ",
        "outputId": "44114933-9fc2-4a97-fdf4-8d4382831644"
      },
      "outputs": [],
      "source": [
        "def convert_chat_with_tools(messages, tools=None, add_generation_prompt = True):\n",
        "    return tokenizer.apply_chat_template(messages, tools=tools, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
        "\n",
        "example = [{'role':'user', 'content':'ì˜¤ëŠ˜ ë‚ ì§œê°€ ì–´ë–»ê²Œ ë¼?'}]\n",
        "print(convert_chat_with_tools(example, tools=tool_desc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PwXIVXY14aZ",
        "outputId": "dbd3b369-ce06-4b75-8750-2e4f7b432db4"
      },
      "outputs": [],
      "source": [
        "for s in base_llm.stream(convert_chat_with_tools(example, tools=tool_desc)):\n",
        "    print(s, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoA3jD4d14aZ"
      },
      "source": [
        "í•´ë‹¹ ë‚´ìš©ì„ ê°€ì ¸ê°€ì„œ, ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNMMxgw414aZ"
      },
      "outputs": [],
      "source": [
        "# ìµœìƒì˜ ê²°ê³¼ë¥¼ ìœ„í•´, í† í¬ë‚˜ì´ì €ì˜ í…œí”Œë¦¿ì„ ìµœëŒ€í•œ ë”°ë¦…ë‹ˆë‹¤.\n",
        "system_prompt = f'''\n",
        "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
        "\n",
        "# Tools\n",
        "\n",
        "You may call one or more functions to assist with the user query.\n",
        "\n",
        "You are provided with function signatures within <tools></tools> XML tags:\n",
        "<tools>\n",
        "{tool_desc}\n",
        "</tools>\n",
        "\n",
        "For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
        "<tool_call>\n",
        "{{\"name\": <function-name>, \"arguments\": <args-json-object>}}\n",
        "</tool_call>\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqCmg9EF14aZ"
      },
      "source": [
        "Gemmaì™€ ë™ì¼í•˜ê²Œ ì‹¤í–‰í•´ ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdb8FThu14aZ",
        "outputId": "633da78d-c5e7-40a4-d085-e6064965ad78"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
        "\n",
        "messages = [SystemMessage(system_prompt),\n",
        "            HumanMessage('ì˜¤ëŠ˜ ë‚ ì§œê°€ ë©°ì¹ ì´ë‹ˆ?')]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRZQrsfB14aZ"
      },
      "source": [
        "tool_callì˜ í˜•ì‹ì— ë§ì¶° ì½”ë“œë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b-20HsO14aZ",
        "outputId": "8a5fc277-17d8-46c9-a483-1e4e86307f69"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "def parse_tool(text):\n",
        "    try:\n",
        "        text = text.split('<tool_call>\\n')[1].split('\\n</tool_call>')[0]\n",
        "        # tool_codeë¡œ wrapëœ ì¤‘ê°„ ì½”ë“œ ì¶”ì¶œ\n",
        "\n",
        "        parsed = ast.literal_eval(text)\n",
        "        # Dict í˜•íƒœì˜ ê°’ ë³€í™˜ (json loadì™€ ìœ ì‚¬)\n",
        "        name = parsed.get('name')\n",
        "        arguments = parsed.get('arguments', {})\n",
        "        # nameê³¼ argument return\n",
        "        return {'name':name, 'arguments':arguments}\n",
        "    except (ValueError, SyntaxError):\n",
        "        return None\n",
        "\n",
        "result = parse_tool(response.content)\n",
        "name,arguments = result['name'], result['arguments']\n",
        "name, arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSUaIpCr14aZ"
      },
      "source": [
        "íˆ´ì„ ì‹¤í–‰í•˜ê³ , ê·¸ ê²°ê³¼ëŠ” ToolMessageë¡œ ì „ë‹¬í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5tRRRDpr14aZ",
        "outputId": "dd872cd3-fbf3-43a7-aa08-a22feec9536a"
      },
      "outputs": [],
      "source": [
        "# íˆ´ ì´ë¦„ê³¼ íˆ´ ì—°ê²°\n",
        "tool_dict = {tool.name: tool for tool in tools}\n",
        "\n",
        "def execute_tool(name, arguments):\n",
        "    # íˆ´ ì‹¤í–‰í•œ ë’¤ tool_outputìœ¼ë¡œ wrap\n",
        "    result = tool_dict[name].invoke(arguments)\n",
        "    return str(result)\n",
        "\n",
        "tool_result = execute_tool(**parse_tool(response.content))\n",
        "\n",
        "tool_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ShknVlD14aZ",
        "outputId": "08d8b086-480d-47b7-c8b0-daaca71e5e1c"
      },
      "outputs": [],
      "source": [
        "messages.append(HumanMessage(tool_result))\n",
        "messages[1:]\n",
        "# ì§ˆë¬¸ + Tool ìš”ì²­ + Tool ê²°ê³¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3UQ30AX14aZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfiQVBqt14aZ",
        "outputId": "fbdfd2e8-709e-416c-8234-d6962986cb32"
      },
      "outputs": [],
      "source": [
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZM__QJI14aa"
      },
      "source": [
        "ë™ì¼í•œ êµ¬ì¡°ë¡œ, í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ì—¬ ì „ì²´ ì—ì´ì „íŠ¸ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-giuavz14aa"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages : Annotated[list, add_messages]   # ë©”ì‹œì§€ ë§¥ë½ì„ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_ihlip114aa"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "tool_list = {tool.name: tool for tool in tools}\n",
        "# tool ëª©ë¡ dictë¡œ ìƒì„±\n",
        "\n",
        "def tool_node(state):\n",
        "    tool_outputs = []\n",
        "    tool_call_msgs = state['messages'][-1]\n",
        "    # ë§ˆì§€ë§‰ ë©”ì‹œì§€: íˆ´ ì½œë§ ë©”ì‹œì§€\n",
        "    if '<tool_call>' in tool_call_msgs.content:\n",
        "        tool_result = execute_tool(**parse_tool(tool_call_msgs.content))\n",
        "        # tool ì‹¤í–‰ ê²°ê³¼ ì–»ê¸° (ê²°ê³¼ëŠ” ```tool_output```)\n",
        "        tool_outputs.append(HumanMessage(tool_result))\n",
        "\n",
        "    return {'messages': tool_outputs}\n",
        "\n",
        "def agent(state):\n",
        "    system_prompt = SystemMessage(f'''\n",
        "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
        "\n",
        "# Tools\n",
        "\n",
        "You may call a function to assist with the user query.\n",
        "\n",
        "You are provided with function signatures within <tools></tools> XML tags:\n",
        "<tools>\n",
        "{tool_desc}\n",
        "</tools>\n",
        "\n",
        "For function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
        "<tool_call>\n",
        "{{\"name\": <function-name>, \"arguments\": <args-json-object>}}\n",
        "</tool_call>\n",
        "\n",
        "# Tool Usage Rules - VERY IMPORTANT!\n",
        "\n",
        "1.  **Analyze the Task:** Carefully read the user's request and determine if any tools are needed.\n",
        "2.  **One Tool At A Time:** If you need to use a tool, you **MUST** choose and call **only ONE** tool in your response. Do **NOT** issue multiple `<tool_call>` tags in a single turn.\n",
        "3.  **Sequential Execution:** If the user's request requires information from multiple tool calls (e.g., searching for information and then getting the weather based on the search result), you **MUST** perform these calls sequentially.\n",
        "    * First, call the **single** tool needed for the first step.\n",
        "    * Wait for the result of that tool.\n",
        "    * Then, based on the result, decide if another **single** tool call is necessary for the next step. Issue that call in a *new* response turn.\n",
        "\n",
        "Answer in Korean.''')\n",
        "\n",
        "\n",
        "    response = llm.invoke([system_prompt] + state[\"messages\"])\n",
        "    return {'messages': response}\n",
        "\n",
        "def tool_needed(state):\n",
        "\n",
        "    last_msg = state['messages'][-1]\n",
        "    if '<tool_call>' in last_msg.content: # íˆ´ ì½œë§ì´ í•„ìš”í•˜ë©´\n",
        "        return \"continue\"\n",
        "    else:\n",
        "        return \"finish\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpYW7TQ314aa",
        "outputId": "81f36951-15e5-475e-dfc3-73e9285e04cd"
      },
      "outputs": [],
      "source": [
        "# ë©”ëª¨ë¦¬ ì„¸íŒ…\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"agent\", agent)\n",
        "builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "builder.add_edge(START, 'agent'),\n",
        "builder.add_conditional_edges(\"agent\",\n",
        "                              tool_needed,\n",
        "                               {\"continue\": \"tools\",\"finish\": END})\n",
        "builder.add_edge(\"tools\", \"agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsuR8tZd14aa",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "memory = MemorySaver()\n",
        "graph = builder.compile(checkpointer = memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIHSRouc14aa",
        "outputId": "01b6ca20-d74d-4805-ce85-d4c849a7d87c"
      },
      "outputs": [],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "\n",
        "response = graph.invoke({'messages':[HumanMessage(content=\"ì˜¤ëŠ˜ì´ ë©°ì¹ ì´ì•¼?\")]}, config)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llXZkL_714aa",
        "outputId": "6a15ace9-f62a-4a6e-f8bf-c8d6cb176a76"
      },
      "outputs": [],
      "source": [
        "# sLLM: ê²°ê³¼ê°€ Inconsistent...\n",
        "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "response = graph.invoke({'messages':[HumanMessage(content=\"ì˜¤ëŠ˜ê³¼ ê°™ì€ ë‚ ì§œì— íƒœì–´ë‚œ ìœ ëª…ì¸ë“¤ ì¡°ì‚¬í•´ì„œ ì•Œë ¤ì¤˜.\")]}, config)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3opOZPT-14aa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlt75bEw14aa",
        "outputId": "f81775e1-b011-4b3c-df11-488e584d7656"
      },
      "outputs": [],
      "source": [
        "# ê²€ìƒ‰ 2\n",
        "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
        "\n",
        "response = graph.invoke({'messages':[HumanMessage(content=\"2025ë…„ 4ì›”ì— ì¶œì‹œëœ GPT-4.1 ëª¨ë¸ì— ëŒ€í•´ ì†Œê°œí•´ì¤˜.\")]},\n",
        "                       config)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRbTmsmI14aa"
      },
      "source": [
        "Toolì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” Agentic Workì˜ ê²½ìš°ì—ëŠ” ê¸°ì¡´ LLMê³¼ ë™ì¼í•˜ê²Œ ì‹¤í–‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruxvj95514aa"
      },
      "source": [
        "ë‹¨, Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ê²½ìš° ë™ì‹œ ì‹¤í–‰ ë“±ì˜ ë©”ì»¤ë‹ˆì¦˜ì´ ìµœì í™”ë˜ì–´ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì—,   \n",
        "ì´ì „ì˜ Send()ì™€ ê°™ì´ ë³‘ë ¬ ì‹¤í–‰ì´ í•„ìš”í•œ ë¬¸ì œì—ì„œëŠ” ê²°ê³¼ê°€ ì œëŒ€ë¡œ ë‚˜ì˜¤ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.    \n",
        "\n",
        "ì´ë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ì€, Ollamaë‚˜ vLLMê³¼ ê°™ì€ ì„œë¹™ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "26999a37e67148c299927987210755b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_588182a5c01f4f81883486dc74b6469f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a1deaade948a4ff6acb34df5ac5f26dd",
            "value": "â€‡4/4â€‡[01:22&lt;00:00,â€‡19.82s/it]"
          }
        },
        "33e9fdbd4cc94f7a9dad92a150046c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5369d4cdad1e48a696c4545b36023c75": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "588182a5c01f4f81883486dc74b6469f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bdaf67b2c3042f7a9058b0a7262114d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7878d799f8074aa3a8fc164c0daeab04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fcd670f11ba4e218f009ce0baa01eef",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33e9fdbd4cc94f7a9dad92a150046c8e",
            "value": 4
          }
        },
        "7fcd670f11ba4e218f009ce0baa01eef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98c5923efc05402fa4b06d32ad1588ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_daf5a315673147779de0b999a7d2315b",
              "IPY_MODEL_7878d799f8074aa3a8fc164c0daeab04",
              "IPY_MODEL_26999a37e67148c299927987210755b4"
            ],
            "layout": "IPY_MODEL_9de1e5c8a8b04dc1891ec35e400e3ced"
          }
        },
        "9de1e5c8a8b04dc1891ec35e400e3ced": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1deaade948a4ff6acb34df5ac5f26dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daf5a315673147779de0b999a7d2315b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5369d4cdad1e48a696c4545b36023c75",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6bdaf67b2c3042f7a9058b0a7262114d",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
