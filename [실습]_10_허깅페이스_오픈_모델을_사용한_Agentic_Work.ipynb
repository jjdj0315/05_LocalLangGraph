{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsYrzxX5DaqK"
      },
      "source": [
        "# [실습] HuggingFace 공개 LLM을 활용한 Agentic Work 만들기\n",
        "\n",
        "HuggingFace에 게시된 공개 모델들의 경우, `bind_tools`와 같은 기능들이 제대로 연동되지 않거나,   \n",
        "자체적으로 Tool Calling 기능이 없는 경우가 많습니다.   \n",
        "\n",
        "이를 해결하기 위해, Tool Calling 방법을 직접 구성하고 Agentic Work를 구현해 보겠습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiuZNYFq14aN"
      },
      "source": [
        "## 라이브러리 설치\n",
        "\n",
        "\n",
        "이번 실습은 무료 코랩(T4 GPU)이 아닌 고성능 GPU 라이브러리에서 진행합니다.   \n",
        "무료 코랩으로 진행하시는 경우, GPU 성능의 한계로 실행이 오래 걸릴 수 있습니다.   \n",
        "(아래 코드에서, **T4 사용시 해제** 부분을 참고하세요!)\n",
        "\n",
        "<br><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Udtv--pI14aN",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers bitsandbytes accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1JC3bK9DaqL"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade langchain_experimental langgraph langchain langchain_community langchain_huggingface dotenv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1eMoe-G14aO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gYEfMXn14aO",
        "outputId": "276917bd-6172-4f0f-f252-d3126f7288fb"
      },
      "outputs": [],
      "source": [
        "# Flash Attention: 리눅스 전용 설치방법\n",
        "# 무료 코랩(T4)에서는 설치 X\n",
        "\n",
        "# Windows 설치는 https://github.com/kingbri1/flash-attention/releases 참고\n",
        "!pip install flash-attn --no-build-isolation -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DtuBpmP14aO"
      },
      "source": [
        "설치할 라이브러리가 많으므로, 가급적 설치 후 세션 재시작을 수행해 주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBhW7vFc14aO"
      },
      "source": [
        "허깅페이스 토큰을 인증합니다.    \n",
        "\n",
        "https://huggingface.co/settings/tokens 에서 Read 권한 토큰을 발급받습니다.   \n",
        "아래 코드에서는, env 파일에 `HF_READ_TOKEN`으로 저장했습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKRxTZ5n14aO"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVFaB-AA14aO",
        "outputId": "047ac881-646b-4d8f-93d6-a848a719e239"
      },
      "outputs": [],
      "source": [
        "load_dotenv('env') #.env 파일인 경우 .env로 변경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "siIa3Voy14aO",
        "outputId": "04a4ff69-9a66-4687-fb4f-b3733570a545"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJEt8RpX14aO",
        "outputId": "0194c900-3ae1-479a-f954-2e64d8a5d1e8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "# 허깅페이스 토큰 로그인\n",
        "login(token=os.environ['HF_TOKEN'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d__aPk5_14aO"
      },
      "source": [
        "### 1. Gemma 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrzEKTP514aP"
      },
      "source": [
        "불러올 모델은 구글의 Gemma-3-12b-it 입니다.   \n",
        "실제 크기는 24GB 정도이지만, 4Bit 양자화를 통해 8~9GB 크기로 로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6QY8RHv14aP",
        "outputId": "a5be359d-d34b-48ba-c51e-99da2394aa45"
      },
      "outputs": [],
      "source": [
        "model_id = \"google/gemma-3-12b-it\"\n",
        "print(f\"# Model ID: {model_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE5W5Bat14aP"
      },
      "source": [
        "트랜스포머 라이브러리를 이용해 모델을 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z592IDzIDaqM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# 양자화 옵션: 4비트\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    # nf4 양자화\n",
        "\n",
        "    bnb_4bit_compute_dtype=\"bfloat16\", # 계산시에는 기존 자료형 bf16 사용\n",
        "    bnb_4bit_use_double_quant=True # 이중 양자화\n",
        ")\n",
        "\n",
        "\n",
        "# 토크나이저와 모델 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "\n",
        "    torch_dtype=torch.bfloat16, # Gemma 3은 bf16 모델입니다!\n",
        "    device_map={\"\":0},  # 0번 GPU에 할당\n",
        "\n",
        "    quantization_config = quantization_config,\n",
        "    # 양자화 불필요시 제거\n",
        "\n",
        "    attn_implementation = 'eager'\n",
        "    # flash attention 설정\n",
        "    # Gemma Series가 아닌 경우 'eager'를 \"flash_attention_2\" 로 변경\n",
        "    # 미지원/사용하지 않는 경우 제거 (T4 등의 구버전 GPU 등)\n",
        "\n",
        ")\n",
        "\n",
        "# if hasattr(model, 'language_model'):\n",
        "#     model = model.language_model\n",
        "# Multimodal 모델의 경우, Language Model만 선택할 수도 있으나\n",
        "# 비전 모델의 사이즈가 매우 작기 때문에 큰 차이는 없음\n",
        "\n",
        "# Train X (eval)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRYqBlyQ14aP"
      },
      "source": [
        "모델을 불러온 뒤에는 Text-Generation 파이프라인을 구성합니다.   \n",
        "이 때, 적절한 샘플링 파라미터를 설정하기 위해 모델 홈페이지를 참고할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gFbGhuK14aP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
        "\n",
        "# Sampling 파라미터 설정: https://huggingface.co/google/gemma-3-12b-it\n",
        "# 홈페이지 권장 사양이지만, 적절하게 바꿔도 됨\n",
        "gen_config = dict(\n",
        "    do_sample=True,\n",
        "    max_new_tokens=2048,\n",
        "    repetition_penalty = 1.05,\n",
        "\n",
        "    temperature = 1.0,\n",
        "    top_p = 0.95,\n",
        "    top_k = 64\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    # return_full_text=True <-- 입력 프롬프트를 포함한 전체 출력하기\n",
        "    **gen_config\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a-z0Pbt14aP"
      },
      "source": [
        "모델의 토크나이저를 확인하여, 채팅 템플릿 구성을 확인합니다.   \n",
        "전체를 이해할 필요는 없지만, Tool이나 Function이 있는지 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRHzfqUk14aP"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.chat_template)\n",
        "# Tool 미지원 😣"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqWVrzLe14aP"
      },
      "source": [
        "모델을 랭체인과 연동합니다.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZsarwsm14aP"
      },
      "outputs": [],
      "source": [
        "base_llm = HuggingFacePipeline(pipeline=pipe, pipeline_kwargs=gen_config)\n",
        "# base_llm: 입력 전 Tokenizer로 템플릿 변환 필수, 스트리밍 가능\n",
        "\n",
        "llm = ChatHuggingFace(llm=base_llm, tokenizer=tokenizer)\n",
        "# llm: 자동 템플릿 변환(ChatGoogleGenAI와 동일), 스트리밍 불가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPs0pT9z14aP"
      },
      "outputs": [],
      "source": [
        "def convert_chat(messages, add_generation_prompt = True):\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
        "\n",
        "example = [{'role':'user', 'content':'안녕'}]\n",
        "convert_chat(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tf93bsuV14aQ"
      },
      "outputs": [],
      "source": [
        "for s in base_llm.stream(convert_chat(example)):\n",
        "    print(s, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG9dLxzP14aQ"
      },
      "outputs": [],
      "source": [
        "llm.invoke(\"안녕?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwnCxMeP14aQ"
      },
      "source": [
        "## 2. HuggingFace LLM과 툴 연동하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc0lmLMh14aQ"
      },
      "source": [
        "먼저 툴을 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxxyvxZn14aQ"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.tools import tool\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from datetime import datetime\n",
        "\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=3)\n",
        "\n",
        "repl_tool = PythonREPLTool()\n",
        "repl_tool.invoke(\"for i in range(10): print(i)\")\n",
        "\n",
        "\n",
        "@tool\n",
        "def current_date() -> str:\n",
        "    \"현재 날짜를 %y-%m-%d 형식으로 반환합니다.\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "\n",
        "tools = [tavily_search, repl_tool, current_date]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPAdZG1H14aQ"
      },
      "source": [
        "`ChatHuggingFace()`에도 `bind_tools()`이 구현되어 있으나,   \n",
        "실제로 실행되지 않는 경우가 많습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZoPFhiN14aQ"
      },
      "outputs": [],
      "source": [
        "llm_with_tools = llm.bind_tools(tools)\n",
        "llm_with_tools.invoke(\"오늘 날짜가 어떻게 되니?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwvRgFfO14aQ"
      },
      "outputs": [],
      "source": [
        "llm_with_tools.invoke(\"너는 어떤 툴이나 함수를 가지고 있니?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rP-lWBN14aQ"
      },
      "source": [
        "이런 경우, 툴을 실행하기 위해서는 직접 시스템 메시지를 구성해야 합니다.   \n",
        "Tool 역할 또한 존재하지 않으므로 User 역할을 툴 대용으로 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBkBjlRE14aQ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "llm_with_tools.kwargs['tools']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pirXIlE14aQ"
      },
      "source": [
        "툴 설명이 담긴 문자열을 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAvTnfq614aQ"
      },
      "outputs": [],
      "source": [
        "tool_desc = str('\\n---\\n'.join([str(x) for x in llm_with_tools.kwargs['tools']]))\n",
        "print(tool_desc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AaY3Syl14aQ"
      },
      "source": [
        "시스템 프롬프트를 구성합니다.   \n",
        "성능에 매우 중요한 영향을 미치므로, 영어로 작성했습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMNnrs6s14aQ"
      },
      "outputs": [],
      "source": [
        "system_prompt = f'''\n",
        "You are a helpful assistant with tools below.\n",
        "You can decide whether to invoke any functions or not.\n",
        "If you decide to use any of tools.\n",
        "print name and required parameters of the tool within a json blob correctly.\n",
        "For python code, Return the object as a raw dictionary, without escaping quotes or newlines.\n",
        "\n",
        "for tool use: wrap your output within ```tool_code```.\n",
        "\n",
        "Example:\n",
        "```tool_code\n",
        "{{\"name\":'name of tool', \"arguments\":{{List of apparent argument and parameters}}}}\n",
        "```\n",
        "\n",
        "When the output of the tool is provided, it will be wrapped within ``tool_output```\n",
        "Answer accordingly from the result of the tool output.\n",
        "\n",
        "The question might need some sequential, multiple tool execution.\n",
        "Think Step by Step.\n",
        "\n",
        "The following tools are available:\n",
        "{tool_desc}'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl0MkeqQ14aR"
      },
      "source": [
        "시스템 프롬프트에 들어가야 하는 내용은 다음과 같습니다.\n",
        "- Tool Format\n",
        "- Tool Call Format\n",
        "- Tool Result Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLS6guf414aV"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
        "\n",
        "\n",
        "messages = [SystemMessage(system_prompt),\n",
        "            HumanMessage('오늘 날짜가 며칠이니?')]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izjgV8rS14aW"
      },
      "outputs": [],
      "source": [
        "\n",
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVRGa7X914aW"
      },
      "outputs": [],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6okfSfZe14aW"
      },
      "source": [
        "tool_code를 받았으니, 해당 내용을 파싱합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnDI-EaK14aW"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "\n",
        "def parse_tool(text):\n",
        "    try:\n",
        "        text = text.split('```tool_code\\n')[1].split('\\n```')[0]\n",
        "        # tool_code로 wrap된 중간 코드 추출\n",
        "\n",
        "        parsed = ast.literal_eval(text)\n",
        "        # Dict 형태의 값 변환 (json load와 유사)\n",
        "\n",
        "        name = parsed.get('name')\n",
        "        arguments = parsed.get('arguments', {})\n",
        "        # name과 argument return\n",
        "        return {'name':name, 'arguments':arguments}\n",
        "    except (ValueError, SyntaxError):\n",
        "        return None\n",
        "\n",
        "result = parse_tool(response.content)\n",
        "name,arguments = result['name'], result['arguments']\n",
        "name, arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgwuDSRE14aW"
      },
      "source": [
        "툴 실행을 연결합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfNnf7x514aW"
      },
      "outputs": [],
      "source": [
        "# 툴 이름과 툴 연결\n",
        "tool_dict = {tool.name: tool for tool in tools}\n",
        "\n",
        "def execute_tool(name, arguments):\n",
        "    # 툴 실행한 뒤 tool_output으로 wrap\n",
        "    result = f'''```tool_output\n",
        "{tool_dict[name].invoke(arguments)}\n",
        "```'''\n",
        "    return result\n",
        "\n",
        "tool_result = execute_tool(**parse_tool(response.content))\n",
        "\n",
        "print(tool_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImCgtT-714aW"
      },
      "outputs": [],
      "source": [
        "messages.append(HumanMessage(tool_result))\n",
        "messages[1:]\n",
        "# 질문 + Tool 요청 + Tool 결과"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIPtqA1514aW"
      },
      "outputs": [],
      "source": [
        "# 결과 해석\n",
        "response = llm.invoke(messages)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPtixRcX14aW"
      },
      "outputs": [],
      "source": [
        "messages = [SystemMessage(system_prompt),\n",
        "            HumanMessage('2025년 4월 발표된 GPT-4.1 모델이 어떤 모델이야? 한국어로 설명해줘.')]\n",
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdLc7MVO14aW"
      },
      "outputs": [],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIOlRjoY14aW"
      },
      "outputs": [],
      "source": [
        "tool_result = execute_tool(**parse_tool(response.content))\n",
        "tool_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg51FbMB14aW"
      },
      "outputs": [],
      "source": [
        "messages[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwbD7jk214aX"
      },
      "outputs": [],
      "source": [
        "messages.append(HumanMessage(tool_result))\n",
        "response = llm.invoke(messages)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gojp1ieM14aX"
      },
      "source": [
        "일반적인 입출력 관계의 툴은 이와 같은 방식으로 간단하게 구성할 수 있습니다.   \n",
        "(만약, Python_REPL과 같이 argument가 복잡한 툴을 수행하는 경우에는   \n",
        "별도의 함수로 변환하거나 결과물을 수정하는 작업이 필요할 수 있습니다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEPVMsmk14aX"
      },
      "source": [
        "해당 구현을 통해, ReAct Agent 구조를 만들어 보겠습니다.    \n",
        "bind_tools가 없기 때문에, 기존의 Tool Message를 사용하기 어렵습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP38zKqB14aX"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages : Annotated[list, add_messages]   # 메시지 맥락을 저장하는 리스트\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur7uj1AG14aX"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "tool_list = {tool.name: tool for tool in tools}\n",
        "# tool 목록 dict로 생성\n",
        "\n",
        "def tool_node(state):\n",
        "    tool_outputs = []\n",
        "    tool_call_msgs = state['messages'][-1]\n",
        "    # 마지막 메시지: 툴 콜링 메시지\n",
        "    if '```tool_code' in tool_call_msgs.content:\n",
        "        tool_result = execute_tool(**parse_tool(tool_call_msgs.content))\n",
        "        # tool 실행 결과 얻기 (결과는 ```tool_output```)\n",
        "        tool_outputs.append(HumanMessage(tool_result))\n",
        "\n",
        "    return {'messages': tool_outputs}\n",
        "\n",
        "def agent(state):\n",
        "    system_prompt = SystemMessage(f'''\n",
        "You are a helpful assistant with tools below.\n",
        "You can decide whether to invoke any functions or not.\n",
        "If you decide to use any of tools.\n",
        "print name and required parameters of the tool within a json blob correctly.\n",
        "For python code, Return the object as a raw dictionary, without escaping quotes or newlines.\n",
        "\n",
        "for tool use: wrap your output within ```tool_code```.\n",
        "\n",
        "Example:\n",
        "```tool_code\n",
        "{{\"name\":'name of tool', \"arguments\":{{List of apparent argument and parameters}}}}\n",
        "```\n",
        "\n",
        "When the output of the tool is provided, it will be wrapped within ``tool_output```\n",
        "Answer accordingly from the result of the tool output.\n",
        "\n",
        "The question might need some sequential, multiple tool execution.\n",
        "Think Step by Step.\n",
        "\n",
        "The following tools are available:\n",
        "{tool_desc}\n",
        "\n",
        "\n",
        "Answer in Korean.''')\n",
        "\n",
        "\n",
        "    response = llm.invoke([system_prompt] + state[\"messages\"])\n",
        "    return {'messages': response}\n",
        "\n",
        "def tool_needed(state):\n",
        "\n",
        "    last_msg = state['messages'][-1]\n",
        "    if '```tool_code' in last_msg.content: # 툴 콜링이 필요하면\n",
        "        return \"continue\"\n",
        "    else:\n",
        "        return \"finish\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tgTNu8f14aX"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"agent\", agent)\n",
        "builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "builder.add_edge(START, 'agent'),\n",
        "builder.add_conditional_edges(\"agent\",\n",
        "                              tool_needed,\n",
        "                               {\"continue\": \"tools\",\"finish\": END})\n",
        "builder.add_edge(\"tools\", \"agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIaHRhJq14aX"
      },
      "outputs": [],
      "source": [
        "graph = builder.compile()\n",
        "graph # 생성한 그래프 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwtyHK-C14aX"
      },
      "outputs": [],
      "source": [
        "response = graph.invoke({'messages':[HumanMessage(content=\"오늘 날짜에 태어난 유명인들 조사해서 알려줘.\")]})\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gDQQHN_14aX"
      },
      "source": [
        "Gemma 3는 Native Tool Call을 지원하는 모델은 아닙니다.  \n",
        "하지만, 어느 정도 함수 실행 능력을 보이는데요.   \n",
        "\n",
        "마찬가지로, Gemma 3 이외의 LLM 모델을 사용하는 경우에도,  \n",
        "각각의 Tool 스펙에 대한 문서를 참고하여 진행하실 수 있으나, 성능은 조금 떨어질 수 있습니다.   \n",
        "Ex) Phi-4 https://huggingface.co/microsoft/Phi-4-mini-instruct\n",
        "\n",
        "<br><br>\n",
        "이번에는 Qwen 2.5 7B를 이용해 진행해 보겠습니다.\n",
        "\n",
        "**세션 초기화 후 여기서부터 다시 시작해 주세요!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSelbmwB14aX",
        "outputId": "13751504-363e-4803-8251-dae0429caee1"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv('env')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxqnxWpk14aX",
        "outputId": "0b5b7820-8e92-44a6-f548-9bd2f2042406"
      },
      "outputs": [],
      "source": [
        "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "print(f\"# Model ID: {model_id}\")\n",
        "# Full Precision 로드: 15.6GB\n",
        "# GPU 부족시 양자화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KNPHUQ5_rKt"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# 양자화 옵션: 4비트\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    # nf4 양자화\n",
        "\n",
        "    bnb_4bit_compute_dtype=\"bfloat16\", # 계산시에는 기존 자료형 bf16 사용\n",
        "    bnb_4bit_use_double_quant=True # 이중 양자화\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668,
          "referenced_widgets": [
            "98c5923efc05402fa4b06d32ad1588ef",
            "daf5a315673147779de0b999a7d2315b",
            "7878d799f8074aa3a8fc164c0daeab04",
            "26999a37e67148c299927987210755b4",
            "9de1e5c8a8b04dc1891ec35e400e3ced",
            "5369d4cdad1e48a696c4545b36023c75",
            "6bdaf67b2c3042f7a9058b0a7262114d",
            "7fcd670f11ba4e218f009ce0baa01eef",
            "33e9fdbd4cc94f7a9dad92a150046c8e",
            "588182a5c01f4f81883486dc74b6469f",
            "a1deaade948a4ff6acb34df5ac5f26dd"
          ]
        },
        "id": "RnflkPZR14aY",
        "outputId": "c5833b90-b1c1-495c-f1f0-121ed8fd24dd"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# 토크나이저와 모델 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "\n",
        "    torch_dtype='auto',\n",
        "    device_map={\"\":0},  # 0번 GPU에 할당\n",
        "\n",
        "    quantization_config  = quantization_config,\n",
        "\n",
        "    # attn_implementation = 'flash_attention_2'\n",
        "    # flash attention 설정\n",
        "    # 미지원/사용하지 않는 경우 제거 (T4 등의 구버전 GPU 등)\n",
        "\n",
        ")\n",
        "\n",
        "# Train X (eval)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZshkwSU14aY",
        "outputId": "565ac542-e095-4101-a1ec-45ab0fd9848b"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
        "\n",
        "# Sampling 파라미터 설정: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct\n",
        "# 홈페이지 권장 사양이지만, 적절하게 바꿔도 됨\n",
        "gen_config = dict(\n",
        "    do_sample=True,\n",
        "    max_new_tokens=2048,\n",
        "    repetition_penalty = 1.05,\n",
        "\n",
        "    temperature = 0.1,\n",
        "    top_p = 0.8,\n",
        "    top_k = 20\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    # return_full_text=True <-- 입력 프롬프트를 포함한 전체 출력하기\n",
        "    **gen_config\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpGHbrzF14aY"
      },
      "source": [
        "Qwen 2.5 는 Tool 기능이 포함되어 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsDwcaHq14aY",
        "outputId": "5f4f271d-638b-4816-f2af-ac46cad9a71f"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.chat_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDoXXYMK14aY"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.tools import tool\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from datetime import datetime\n",
        "\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=3)\n",
        "\n",
        "repl_tool = PythonREPLTool()\n",
        "\n",
        "\n",
        "@tool\n",
        "def current_date() -> str:\n",
        "    \"현재 날짜를 %y-%m-%d 형식으로 반환합니다.\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "\n",
        "tools = [tavily_search, repl_tool, current_date]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTcLxWoK14aY"
      },
      "outputs": [],
      "source": [
        "base_llm = HuggingFacePipeline(pipeline=pipe, pipeline_kwargs=gen_config)\n",
        "# base_llm: 입력 전 Tokenizer로 템플릿 변환 필수, 스트리밍 가능\n",
        "\n",
        "llm = ChatHuggingFace(llm=base_llm, tokenizer=tokenizer)\n",
        "# llm: 자동 템플릿 변환(ChatGoogleGenAI와 동일), 스트리밍 불가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZhrctni14aY"
      },
      "source": [
        "툴 기능이 있어도, bind_tools는 사용이 어렵습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMKVBn3-14aY",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "llm_with_tools = llm.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BXWUzvg14aY",
        "outputId": "5ce776cf-867d-4083-ff71-cc350f3df970"
      },
      "outputs": [],
      "source": [
        "llm_with_tools.invoke(\"오늘 날짜가 어떻게 되니? 툴을 사용해서 알려줘.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXByDlQD14aY"
      },
      "source": [
        "툴을 json list로 전달하여 템플릿에 적용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT-XCdOo14aY",
        "outputId": "53c65edf-1a1a-4325-d7a3-36370a8dbeba"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "tool_desc = llm_with_tools.kwargs['tools']\n",
        "print(tool_desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ_N2uSH14aY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sayymRP14aY"
      },
      "source": [
        "토크나이저에 tool을 전달하여, 전반적인 템플릿을 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAYKx-ca14aZ",
        "outputId": "44114933-9fc2-4a97-fdf4-8d4382831644"
      },
      "outputs": [],
      "source": [
        "def convert_chat_with_tools(messages, tools=None, add_generation_prompt = True):\n",
        "    return tokenizer.apply_chat_template(messages, tools=tools, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
        "\n",
        "example = [{'role':'user', 'content':'오늘 날짜가 어떻게 돼?'}]\n",
        "print(convert_chat_with_tools(example, tools=tool_desc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PwXIVXY14aZ",
        "outputId": "dbd3b369-ce06-4b75-8750-2e4f7b432db4"
      },
      "outputs": [],
      "source": [
        "for s in base_llm.stream(convert_chat_with_tools(example, tools=tool_desc)):\n",
        "    print(s, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoA3jD4d14aZ"
      },
      "source": [
        "해당 내용을 가져가서, 시스템 프롬프트를 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNMMxgw414aZ"
      },
      "outputs": [],
      "source": [
        "# 최상의 결과를 위해, 토크나이저의 템플릿을 최대한 따릅니다.\n",
        "system_prompt = f'''\n",
        "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
        "\n",
        "# Tools\n",
        "\n",
        "You may call one or more functions to assist with the user query.\n",
        "\n",
        "You are provided with function signatures within <tools></tools> XML tags:\n",
        "<tools>\n",
        "{tool_desc}\n",
        "</tools>\n",
        "\n",
        "For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
        "<tool_call>\n",
        "{{\"name\": <function-name>, \"arguments\": <args-json-object>}}\n",
        "</tool_call>\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqCmg9EF14aZ"
      },
      "source": [
        "Gemma와 동일하게 실행해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdb8FThu14aZ",
        "outputId": "633da78d-c5e7-40a4-d085-e6064965ad78"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
        "\n",
        "messages = [SystemMessage(system_prompt),\n",
        "            HumanMessage('오늘 날짜가 며칠이니?')]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRZQrsfB14aZ"
      },
      "source": [
        "tool_call의 형식에 맞춰 코드를 수정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b-20HsO14aZ",
        "outputId": "8a5fc277-17d8-46c9-a483-1e4e86307f69"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "def parse_tool(text):\n",
        "    try:\n",
        "        text = text.split('<tool_call>\\n')[1].split('\\n</tool_call>')[0]\n",
        "        # tool_code로 wrap된 중간 코드 추출\n",
        "\n",
        "        parsed = ast.literal_eval(text)\n",
        "        # Dict 형태의 값 변환 (json load와 유사)\n",
        "        name = parsed.get('name')\n",
        "        arguments = parsed.get('arguments', {})\n",
        "        # name과 argument return\n",
        "        return {'name':name, 'arguments':arguments}\n",
        "    except (ValueError, SyntaxError):\n",
        "        return None\n",
        "\n",
        "result = parse_tool(response.content)\n",
        "name,arguments = result['name'], result['arguments']\n",
        "name, arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSUaIpCr14aZ"
      },
      "source": [
        "툴을 실행하고, 그 결과는 ToolMessage로 전달합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5tRRRDpr14aZ",
        "outputId": "dd872cd3-fbf3-43a7-aa08-a22feec9536a"
      },
      "outputs": [],
      "source": [
        "# 툴 이름과 툴 연결\n",
        "tool_dict = {tool.name: tool for tool in tools}\n",
        "\n",
        "def execute_tool(name, arguments):\n",
        "    # 툴 실행한 뒤 tool_output으로 wrap\n",
        "    result = tool_dict[name].invoke(arguments)\n",
        "    return str(result)\n",
        "\n",
        "tool_result = execute_tool(**parse_tool(response.content))\n",
        "\n",
        "tool_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ShknVlD14aZ",
        "outputId": "08d8b086-480d-47b7-c8b0-daaca71e5e1c"
      },
      "outputs": [],
      "source": [
        "messages.append(HumanMessage(tool_result))\n",
        "messages[1:]\n",
        "# 질문 + Tool 요청 + Tool 결과"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3UQ30AX14aZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfiQVBqt14aZ",
        "outputId": "fbdfd2e8-709e-416c-8234-d6962986cb32"
      },
      "outputs": [],
      "source": [
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZM__QJI14aa"
      },
      "source": [
        "동일한 구조로, 프롬프트를 수정하여 전체 에이전트를 구성할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-giuavz14aa"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages : Annotated[list, add_messages]   # 메시지 맥락을 저장하는 리스트\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_ihlip114aa"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "tool_list = {tool.name: tool for tool in tools}\n",
        "# tool 목록 dict로 생성\n",
        "\n",
        "def tool_node(state):\n",
        "    tool_outputs = []\n",
        "    tool_call_msgs = state['messages'][-1]\n",
        "    # 마지막 메시지: 툴 콜링 메시지\n",
        "    if '<tool_call>' in tool_call_msgs.content:\n",
        "        tool_result = execute_tool(**parse_tool(tool_call_msgs.content))\n",
        "        # tool 실행 결과 얻기 (결과는 ```tool_output```)\n",
        "        tool_outputs.append(HumanMessage(tool_result))\n",
        "\n",
        "    return {'messages': tool_outputs}\n",
        "\n",
        "def agent(state):\n",
        "    system_prompt = SystemMessage(f'''\n",
        "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
        "\n",
        "# Tools\n",
        "\n",
        "You may call a function to assist with the user query.\n",
        "\n",
        "You are provided with function signatures within <tools></tools> XML tags:\n",
        "<tools>\n",
        "{tool_desc}\n",
        "</tools>\n",
        "\n",
        "For function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
        "<tool_call>\n",
        "{{\"name\": <function-name>, \"arguments\": <args-json-object>}}\n",
        "</tool_call>\n",
        "\n",
        "# Tool Usage Rules - VERY IMPORTANT!\n",
        "\n",
        "1.  **Analyze the Task:** Carefully read the user's request and determine if any tools are needed.\n",
        "2.  **One Tool At A Time:** If you need to use a tool, you **MUST** choose and call **only ONE** tool in your response. Do **NOT** issue multiple `<tool_call>` tags in a single turn.\n",
        "3.  **Sequential Execution:** If the user's request requires information from multiple tool calls (e.g., searching for information and then getting the weather based on the search result), you **MUST** perform these calls sequentially.\n",
        "    * First, call the **single** tool needed for the first step.\n",
        "    * Wait for the result of that tool.\n",
        "    * Then, based on the result, decide if another **single** tool call is necessary for the next step. Issue that call in a *new* response turn.\n",
        "\n",
        "Answer in Korean.''')\n",
        "\n",
        "\n",
        "    response = llm.invoke([system_prompt] + state[\"messages\"])\n",
        "    return {'messages': response}\n",
        "\n",
        "def tool_needed(state):\n",
        "\n",
        "    last_msg = state['messages'][-1]\n",
        "    if '<tool_call>' in last_msg.content: # 툴 콜링이 필요하면\n",
        "        return \"continue\"\n",
        "    else:\n",
        "        return \"finish\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpYW7TQ314aa",
        "outputId": "81f36951-15e5-475e-dfc3-73e9285e04cd"
      },
      "outputs": [],
      "source": [
        "# 메모리 세팅\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"agent\", agent)\n",
        "builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "builder.add_edge(START, 'agent'),\n",
        "builder.add_conditional_edges(\"agent\",\n",
        "                              tool_needed,\n",
        "                               {\"continue\": \"tools\",\"finish\": END})\n",
        "builder.add_edge(\"tools\", \"agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsuR8tZd14aa",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "memory = MemorySaver()\n",
        "graph = builder.compile(checkpointer = memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIHSRouc14aa",
        "outputId": "01b6ca20-d74d-4805-ce85-d4c849a7d87c"
      },
      "outputs": [],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "\n",
        "response = graph.invoke({'messages':[HumanMessage(content=\"오늘이 며칠이야?\")]}, config)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llXZkL_714aa",
        "outputId": "6a15ace9-f62a-4a6e-f8bf-c8d6cb176a76"
      },
      "outputs": [],
      "source": [
        "# sLLM: 결과가 Inconsistent...\n",
        "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "response = graph.invoke({'messages':[HumanMessage(content=\"오늘과 같은 날짜에 태어난 유명인들 조사해서 알려줘.\")]}, config)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3opOZPT-14aa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlt75bEw14aa",
        "outputId": "f81775e1-b011-4b3c-df11-488e584d7656"
      },
      "outputs": [],
      "source": [
        "# 검색 2\n",
        "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
        "\n",
        "response = graph.invoke({'messages':[HumanMessage(content=\"2025년 4월에 출시된 GPT-4.1 모델에 대해 소개해줘.\")]},\n",
        "                       config)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRbTmsmI14aa"
      },
      "source": [
        "Tool을 사용하지 않는 Agentic Work의 경우에는 기존 LLM과 동일하게 실행이 가능합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruxvj95514aa"
      },
      "source": [
        "단, Transformers 라이브러리의 경우 동시 실행 등의 메커니즘이 최적화되어 있지 않기 때문에,   \n",
        "이전의 Send()와 같이 병렬 실행이 필요한 문제에서는 결과가 제대로 나오지 않을 수 있습니다.    \n",
        "\n",
        "이를 해결하는 방법은, Ollama나 vLLM과 같은 서빙 라이브러리를 사용하는 것입니다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "26999a37e67148c299927987210755b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_588182a5c01f4f81883486dc74b6469f",
            "placeholder": "​",
            "style": "IPY_MODEL_a1deaade948a4ff6acb34df5ac5f26dd",
            "value": " 4/4 [01:22&lt;00:00, 19.82s/it]"
          }
        },
        "33e9fdbd4cc94f7a9dad92a150046c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5369d4cdad1e48a696c4545b36023c75": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "588182a5c01f4f81883486dc74b6469f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bdaf67b2c3042f7a9058b0a7262114d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7878d799f8074aa3a8fc164c0daeab04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fcd670f11ba4e218f009ce0baa01eef",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33e9fdbd4cc94f7a9dad92a150046c8e",
            "value": 4
          }
        },
        "7fcd670f11ba4e218f009ce0baa01eef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98c5923efc05402fa4b06d32ad1588ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_daf5a315673147779de0b999a7d2315b",
              "IPY_MODEL_7878d799f8074aa3a8fc164c0daeab04",
              "IPY_MODEL_26999a37e67148c299927987210755b4"
            ],
            "layout": "IPY_MODEL_9de1e5c8a8b04dc1891ec35e400e3ced"
          }
        },
        "9de1e5c8a8b04dc1891ec35e400e3ced": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1deaade948a4ff6acb34df5ac5f26dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daf5a315673147779de0b999a7d2315b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5369d4cdad1e48a696c4545b36023c75",
            "placeholder": "​",
            "style": "IPY_MODEL_6bdaf67b2c3042f7a9058b0a7262114d",
            "value": "Loading checkpoint shards: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
